{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition with CNNs and Transformer: A Parallel Model\n",
    "\n",
    "**In this notebook, I'm going to build upon my [Intro to Speech Audio Classification repo](https://github.com/IliaZenkov/sklearn-audio-classification) and build a convolutional neural network (CNN) stacked in parallel with a Transformer encoder-decoder network to classify audio data. We're working on the [RAVDESS dataset](https://smartlaboratory.org/ravdess/) to classify emotions from one of 8 classes.** \n",
    "\n",
    "From my previous notebook: \"Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNNs) and Convolutional Neural Networks (CNNs) are excellent DNN candidates for audio data classification: LSTM RNNs because of their excellent ability to interpret sequential data such as the audio waveform represented as a time series; CNNs because features engineered on audio data such as spectrograms have marked resemblance to images, in which CNNs excel at recognizing and discriminating between distinct patterns.\" - Me \n",
    "\n",
    "I'm going to build on that - CNNs are still the hallmark of image classification today, although even in this domain Transformers are beginning to take the main stage: A [2021 ICLR submission: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://openreview.net/forum?id=YicbFdNTTy) claims they've implemented a Transformer for image classification that outperforms a state of the art CNN, and at a much lower computational complexity.\n",
    "\n",
    "In addition to taking inspiration from the above, it's also no longer 2015 - so **as the successor of the LSTM-RNN I'm going to implemenet a Transformer model in parallel with a CNN to try and get sorta-state-of-the-art performance on the RAVDESS dataset.** Let's get to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get imports out of the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import warnings; warnings.filterwarnings('ignore') #matplot lib complains about librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features:\n",
    "\n",
    "Define features as in the previous notebook on this task from my ['sklearn-audio-classification' repo](https://github.com/IliaZenkov/sklearn-audio-classification). That notebook explains the motivation behind the Mel Spectrogram - in short, we're looking for transitions in audible pitch frequencies. \n",
    "\n",
    "**We're doing away with the chromagram and MFCCs - with this model Mel Spectrograms to provide as good an accuracy as using all three, and we don't want extra complexity in a highly parameterized deep neural net such as this one** where we don't absolutely need it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAVDESS native sample rate is 48k\n",
    "sample_rate = 48000\n",
    "\n",
    "def feature_melspectrogram(\n",
    "    waveform, \n",
    "    sample_rate,\n",
    "    fft = 1024,\n",
    "    winlen = 512,\n",
    "    window='hamming',\n",
    "    hop=256,\n",
    "    mels=128,\n",
    "    ):\n",
    "    \n",
    "    # Produce the mel spectrogram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
    "    # Using 8khz as upper frequency bound should be enough for most speech classification tasks\n",
    "    melspectrogram = librosa.feature.melspectrogram(\n",
    "        y=waveform, \n",
    "        sr=sample_rate, \n",
    "        n_fft=fft, \n",
    "        win_length=winlen, \n",
    "        window=window, \n",
    "        hop_length=hop, \n",
    "        n_mels=mels, \n",
    "        fmax=sample_rate/2)\n",
    "    \n",
    "    # convert from power (amplitude**2) to decibels\n",
    "    melspectrogram = librosa.power_to_db(melspectrogram, ref=np.max)\n",
    "    \n",
    "    return melspectrogram\n",
    "\n",
    "def get_features(file):\n",
    "    \n",
    "    # load an individual sample audio file\n",
    "    # read the full 3 seconds of the file, cut off the first 0.5s of silence; native sample rate = 48k\n",
    "    # don't need to store the sample rate that librosa.load returns\n",
    "    waveform, _ = librosa.load(file, duration=3, offset=0.5, sr=sample_rate)\n",
    "    \n",
    "    # make sure waveform vectors are homogenous by defining explicitly\n",
    "    waveform_homo = np.zeros((int(sample_rate*3,)))\n",
    "    waveform_homo[:len(waveform)] = waveform\n",
    "    \n",
    "    # compute spectrogram features from audio waveform                                      \n",
    "    melspectrogram = feature_melspectrogram(waveform_homo, sample_rate)\n",
    "\n",
    "    return melspectrogram, waveform_homo\n",
    "    \n",
    "# RAVDESS dataset emotions\n",
    "emotions_dict ={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "\n",
    "# Additional attributes from RAVDESS to play with\n",
    "emotion_attributes = {\n",
    "    '01': 'normal',\n",
    "    '02': 'strong'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data:\n",
    "\n",
    "We process each file in the dataset and extract its features.\n",
    "\n",
    "We return the features and the labels (from the file names of the RAVDESS audio samples). We also return the raw waveforms because we're going to do some extra processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "data_path = \".\\\\RAVDESS dataset\\\\actor_*\\\\*.wav\"\n",
    "\n",
    "def load_data():\n",
    "    # features and labels\n",
    "    X,y=[],[]\n",
    "    # raw waveforms to augment later\n",
    "    waveforms = []\n",
    "    # extra labels\n",
    "    y_intensity, y_gender = [],[]\n",
    "    # progress counter\n",
    "    file_count = 0\n",
    "    \n",
    "    for file in glob.glob(data_path):\n",
    "        file_name = os.path.basename(file)\n",
    "        \n",
    "        # get labels of the sample\n",
    "        emotion = emotions_dict[file_name.split(\"-\")[2]]\n",
    "        intensity = emotion_attributes[file_name.split(\"-\")[3]]\n",
    "        if (int((file_name.split(\"-\")[6]).split(\".\")[0]))%2==0: \n",
    "            gender = 'female' \n",
    "        else: \n",
    "            gender = 'male'\n",
    "            \n",
    "        # extract features form the sample\n",
    "        features, waveform = get_features(file)\n",
    "        \n",
    "        # store waveforms to augment \n",
    "        waveforms.append(waveform)\n",
    "        \n",
    "        # store features and data\n",
    "        X.append(features)\n",
    "        y.append(emotion)\n",
    "        y_intensity.append(intensity) # store intensity in case we wish to predict\n",
    "        y_gender.append(gender) # store gender in case we wish to predict \n",
    "        \n",
    "        file_count += 1\n",
    "        # keep track of data loader's progress\n",
    "        print(f' Processed {file_count}/{1440} audio samples',end='\\r')\n",
    "        \n",
    "    # Return arrays to plug into sklearn's cross-validation algorithms\n",
    "    # But need the native dimension of the mel spectrograms list (X) to augment it \n",
    "    return X, waveforms, np.array(y), np.array(y_intensity), np.array(y_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed 1440/1440 audio samples\r"
     ]
    }
   ],
   "source": [
    "features, waveforms, emotions, intensities, genders = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check extracted features and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features set: 1440 samples\n",
      "Features (mel spectrogram) shape: 128 mels x 563 time steps\n",
      "Waveforms set: 1440 samples\n",
      "Waveform signal length: 144000\n",
      "Emotions set: 1440 labels\n",
      "\n",
      "Extra data:\n",
      "Emotion intensity set: 1440 labels\n",
      "Actor gender set: 1440 labels\n"
     ]
    }
   ],
   "source": [
    "print(f'Features set: {len(features)} samples')\n",
    "print(f'Features (mel spectrogram) shape: {len(features[0])} mels x {len(features[0][1])} time steps')\n",
    "print(f'Waveforms set: {len(waveforms)} samples')\n",
    "# we have 1440 waveforms but we need to know their length too; should be 3 sec * 48k = 144k\n",
    "print(f'Waveform signal length: {len(waveforms[0])}')\n",
    "print(f'Emotions set: {emotions.shape[0]} labels')\n",
    "\n",
    "print('\\nExtra data:')\n",
    "print(f'Emotion intensity set: {intensities.shape[0]} labels')\n",
    "print(f'Actor gender set: {genders.shape[0]} labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. 1440 samples, 1440 sets of features and 1440 labels.\n",
    "\n",
    "**Waveforms are 144k long because 3 seconds * 48k sample rate = 144k length array representing the 3 second audio snippet.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting the Data: Additive White Gaussian Noise (AWGN)\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Since our dataset is small, it is prone to overfitting - especially with highly parameterized deep neural net models\n",
    "such as the one we aim to build in this notebook. As such, we're going to want to augment our data. Generating more real samples will be immensely difficult. Instead, we can add white noise to the audio signals - not only to mask the effect of random noise present in the training set - but also **to create pseudo-new training samples and offset the impact of noise intrinsic to the dataset.** \n",
    "\n",
    "In addition, the RAVDESS dataset is extremely clean - we will likely want to make predictions on noisy, real-world data - yet another reason to augment the training data.\n",
    "\n",
    "We're going to use Additive White Gaussian Noise (AWGN). It's Additive because we're adding it to the source audio signal,\n",
    "**it's Gaussian because the noise vector will be sampled from a normal distribution and have a time average of zero (zero-mean), and it's white because after a whitening transformation the noise will add power to the audio signal uniformly across the frequency distribution.**\n",
    "\n",
    "We need a good balance of noise - too little will be useless, and too much will make it too difficult for the network to learn from the training data. **Note that this is just for training - we would _not_ need to add AWGN to real-world data on which we make predictions** (although we could). \n",
    "\n",
    "### Math\n",
    "The key parameters in AWGN are the signal to noise ratio (SNR), defining the magnitude of the noise added w.r.t. the audio signal. We parameterize AWGN with the minimum and maximize SNR so we can pick a random SNR to use in augmenting each sample's waveform.\n",
    "\n",
    "We need to constrain covariance to make it true AWGN. **We make a zero-mean vector of Gaussian noises (np.random.normal) that are statistically dependent. We need to apply a [whitening transformation](https://en.wikipedia.org/wiki/Whitening_transformation)**, a linear transformation taking a vector of random normal (Gaussian) variables with a known covariance matrix and mapping it to a new vector whose covariance is the identity matrix, i.e. the vector is now perfectly uncorrelated with a diaganol covariance matrix, each point of noise having variance == stdev == 1. **The whitening transformation by definition transforms a vector into a white noise vector.**\n",
    "\n",
    "We're going to add the AWGN augmented waveforms as new samples to our dataset. **Since we generate AWGN which is random for each and every sample - random random noise - we can add multiples of our noise-augmented dataset. I'll add 2 extra identical, randomly noisy datasets with 1440 samples each to get a dataset with 1440 native + 1440x2 == 4320 noisy samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def awgn_augmentation(waveform, multiples=2, bits=16, snr_min=15, snr_max=30): \n",
    "    \n",
    "    # get length of waveform (should be 3*48k = 144k)\n",
    "    wave_len = len(waveform)\n",
    "    \n",
    "    # Generate two normally distributed (Gaussian) noises\n",
    "    noise = np.random.normal(size=(multiples, wave_len))\n",
    "    \n",
    "    # Normalize waveform and noise\n",
    "    norm_constant = 2.0**(bits-1)\n",
    "    norm_wave = waveform / norm_constant\n",
    "    norm_noise = noise / norm_constant\n",
    "    \n",
    "    # Compute power of waveform and power of noise\n",
    "    signal_power = np.sum(norm_wave ** 2) / wave_len\n",
    "    noise_power = np.sum(norm_noise ** 2, axis=1) / wave_len\n",
    "    \n",
    "    # Choose random SNR in decibels in range [15,30]\n",
    "    snr = np.random.randint(snr_min, snr_max)\n",
    "    \n",
    "    # Apply whitening transformation: make the Gaussian noise into Gaussian white noise\n",
    "    # Compute the covariance matrix used to whiten each noise \n",
    "    # actual SNR = signal/noise (power)\n",
    "    # actual noise power = 10**(-snr/10)\n",
    "    covariance = np.sqrt((signal_power / noise_power) * 10 ** (- snr / 10))\n",
    "    # Get covariance matrix with dim: (144000, 2) so we can transform 2 noises: dim (2, 144000)\n",
    "    covariance = np.ones((wave_len, multiples)) * covariance\n",
    "\n",
    "    # Take Haddamard product of covariance and noise to generate white noise\n",
    "    # Since covariance and noise are arrays, * is the haddamard product \n",
    "    multiple_augmented_waveforms = waveform + covariance.T * noise\n",
    "    \n",
    "    return multiple_augmented_waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and add the AWGN-augmented audio samples to the rest of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-174-92e382a55b9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# append the augmented spectrogram to the rest of the native data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maugmented_melspectrogram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0memotions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memotions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# keep track of the label we've appended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# keep track of how many waveforms we've processed so we can add correct emotion label in the same order\n",
    "count = 0\n",
    "\n",
    "# specify multiples of our dataaset to add as augmented data\n",
    "multiples = 2\n",
    "\n",
    "for waveform in waveforms:\n",
    "    \n",
    "    # Generate 2 augmented multiples of the dataset, i.e. 1440 native + 1440*2 noisy = 4320 samples total\n",
    "    augmented_waveforms = awgn_augmentation(waveform, multiples=multiples)\n",
    "    \n",
    "    # compute spectrogram for each of 2 augmented waveforms\n",
    "    for augmented_waveform in augmented_waveforms:\n",
    "        \n",
    "        # Compute augmented mel spectrogram\n",
    "        augmented_melspectrogram = feature_melspectrogram(augmented_waveform, sample_rate=sample_rate)\n",
    "\n",
    "        # append the augmented spectrogram to the rest of the native data\n",
    "        features.append(augmented_melspectrogram)\n",
    "        emotions.append(emotions[count])\n",
    "\n",
    "        # keep track of the label we've appended\n",
    "        count += 1\n",
    "        print(f'Processed {count}/{len(waveforms*multiples)} augmented audio samples',end='\\r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check new shape of extracted features and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Native + Augmented Features set: {len(features)} samples')\n",
    "print(f'Features (mel spectrogram) shape: {len(features[0])} mels x {len(features[0][1])} time steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Augmented Waveforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "librosa.display.waveplot(waveforms[0], sr=sample_rate)\n",
    "plt.title('Native')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "librosa.display.waveplot(augmented_waveforms[0], sr=sample_rate)\n",
    "plt.title('AWGN Augmented')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks noisy alright. \n",
    "\n",
    "### Format Data into Tensor-Ready 4D Arrays\n",
    "First stack the 4320-long list of 2D arrays (128x563 mel spectrograms) into a single 3D array.\n",
    "\n",
    "We don't have a colour channel in our mel spectrogram feature array of dim (#samples, mel bands, time steps). **We have an analog of a black and white image: instead of 3 colour channels, we have 1 signal intensity channel: intensity of each of 128 mel frequency bands at time t.**\n",
    "\n",
    "**We need an input channel dim to expand to output channels using CNN filters. We create a dummy channel dim to expand features into 2D-CNN-ready 4D tensor format: N x C x H x W.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the 128 * 563 mel spectrograms into a single N x H x W 3D array  \n",
    "X = np.stack(features, axis=0)\n",
    "\n",
    "# need to make dummy input channel for CNN input feature tensor\n",
    "X = np.expand_dims(X,1)\n",
    "\n",
    "# check we have tensor-ready 4D data array\n",
    "# should print (batch, channel, width, height) == (4320, 1, 128, 563) with multiples==2\n",
    "print('Shape of input 4D feature array: ',X.shape)\n",
    "\n",
    "#free up some memory - no longer need waveforms or individual spectrograms\n",
    "del features, waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll use an 80/10/10 train/validation/test split** to maximize training data and keep a reasonable validaiton/test set. \n",
    "\n",
    "**Have to take care to split the sets proportionally w.r.t. emotion.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define emotion dict without leading 0 for easier/cleaner access\n",
    "emotions_dict ={\n",
    "  '1':'neutral',\n",
    "  '2':'calm',\n",
    "  '3':'happy',\n",
    "  '4':'sad',\n",
    "  '5':'angry',\n",
    "  '6':'fearful',\n",
    "  '7':'disgust',\n",
    "  '8':'surprised'\n",
    "}\n",
    "\n",
    "# create storage for train, validaiton, test sets and their indices\n",
    "train_set,valid_set,test_set = [],[],[]\n",
    "X_train,X_valid,X_test = [],[],[]\n",
    "Y_train,Y_valid,Y_test = [],[],[]\n",
    "\n",
    "# process each 1 of 8 emotions separately so our train/validation/test sets are balanced\n",
    "for emotion in range(1,len(emotions_dict)+1):\n",
    "    \n",
    "    # find all indices of a single unique emotion\n",
    "    emotion_indices = np.where(emotions == emotions_dict[str(emotion)])[0]\n",
    "    # shuffle indices so they're not in order of actor (or model might learn order of actors)\n",
    "    emotion_indices = np.random.permutation(emotion_indices)\n",
    "    # store dim of each emotion set for cleaner code\n",
    "    dim = len(emotion_indices)\n",
    "    \n",
    "    # store indices of training, validation and test sets in 80/10/10 proportion\n",
    "    # train set is first 80%\n",
    "    train_indices = emotion_indices[:int(0.8*dim)]\n",
    "    # validation set is next 10% (between 80% and 90%)\n",
    "    valid_indices = emotion_indices[int(0.8*dim):int(0.9*dim)]\n",
    "    # test set is last 10% (between 90% - end/100%)\n",
    "    test_indices = emotion_indices[int(0.9*dim):]\n",
    "    \n",
    "    # create train features/labels sets\n",
    "    X_train.append(X[train_indices,:,:,:])\n",
    "    Y_train.append(np.array([emotion]*len(train_indices),dtype=np.int32))\n",
    "    # create validation features/labels sets\n",
    "    X_valid.append(X[valid_indices,:,:,:])\n",
    "    Y_valid.append(np.array([emotion]*len(valid_indices),dtype=np.int32))\n",
    "    # create test features/labels sets\n",
    "    X_test.append(X[test_indices,:,:,:])\n",
    "    Y_test.append(np.array([emotion]*len(test_indices),dtype=np.int32))\n",
    "    \n",
    "    train_set.append(train_indices)\n",
    "    valid_set.append(valid_indices)\n",
    "    test_set.append(test_indices)\n",
    "    \n",
    "\n",
    "# create single features array for training, validation, test sets\n",
    "X_train = np.concatenate(X_train,axis=0)\n",
    "X_valid = np.concatenate(X_valid,axis=0)\n",
    "X_test = np.concatenate(X_test,axis=0)\n",
    "\n",
    "# create single emotions/labels array for training, validation, test sets\n",
    "Y_train = np.concatenate(Y_train,axis=0)\n",
    "Y_valid = np.concatenate(Y_valid,axis=0)\n",
    "Y_test = np.concatenate(Y_test,axis=0)\n",
    "\n",
    "# store indices of train, validation, test sets to verify uniqueness\n",
    "train_set = np.concatenate(train_set,axis=0)\n",
    "valid_set = np.concatenate(valid_set,axis=0)\n",
    "test_set = np.concatenate(test_set,axis=0)\n",
    "\n",
    "# check shape of each set\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
    "print(f'X_valid:{X_valid.shape}, Y_valid:{Y_valid.shape}')\n",
    "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')\n",
    "\n",
    "# make sure train, validation, test sets have no overlap/are unique:\n",
    "uniques, dim = np.unique(np.concatenate([train_set,test_set,valid_set],axis=0), return_counts=True)\n",
    "if sum(dim==1) == X.shape[0]:\n",
    "    print(f'Sets are unique: {sum(dim==1)} samples out of {X.shape[0]} are unique')\n",
    "else:\n",
    "    print(f'Sets are NOT unique: {sum(dim==1)} samples out of {X.shape[0]} are unique')\n",
    "\n",
    "# free up memory, no longer need original features/emotions sets\n",
    "del X, emotions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Data\n",
    "Standard Scaling makes the most sense because we have features whose target distribution we don't know. When I performed classification on this dataset with an MLP classifier standard scaling was best even when using just the mel spectrogram feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale the training data\n",
    "# store shape so we can transform it back \n",
    "N,C,H,W = X_train.shape\n",
    "# Reshape: StandardScaler operates on a 1D array\n",
    "# tell numpy to infer shape of 1D array with -1 argument\n",
    "X_train = np.reshape(X_train, (N,-1)) \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Transform back to NxCxHxW 4D tensor format\n",
    "X_train = np.reshape(X_train, (N,C,H,W))\n",
    "\n",
    "# scale the validation set, as above\n",
    "N,C,H,W = X_valid.shape\n",
    "X_valid = np.reshape(X_valid, (N,-1))\n",
    "X_valid = scaler.transform(X_val)\n",
    "X_valid = np.reshape(X_valid, (N,C,H,W))\n",
    "\n",
    "# scale the test set, as above\n",
    "N,C,H,W = X_test.shape\n",
    "X_test = np.reshape(X_test, (N,-1))\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = np.reshape(X_test, (N,C,H,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the terms feature map and tensor interchangeably in the following, although I would generally refer to a feature map as as a batch's Channel + 2D dimensions (C x H x W) and a tensor as the entire training set (N x C x H x W)\n",
    "\n",
    "# 2D Convolutional Layers\n",
    "2d convolution layer is a CNN architecture for images; takes input in a shape (batch size, channel, height, width) i.e #melspecs, 1 channel, height of mel, width of mel). We have 4320 mels, each of shape 128,563 (128 mel frequency bands with 563 timesteps each). takes inspiration from the primary visual cortex\n",
    "\n",
    "## Max Pooling\n",
    "\n",
    "max pooling is a kind of dimensionality reduction technique: it down-samples inputs features , maxpooling in particular uses the maximum of each convolution window which creates a new pooled feature map, i.e. for maxpooling taking into the account the most prominent features of the layer maxpooling reduces the number of parameters a network learns as the connection between sequential layers contains fewer weights because of the reduced dimension in the maxpooled feature map - so computational complexity is drastically reduced. We compute feature map of a convlutional layer with overlapping windowsIf but we maxpool non-overlapping windows based on that feature map to create a down-sampled feature map. intuitively, maxpooling creates a featuremap that takes  the most important feature in each region of the input feature space and puts it in context of its position relative to other most important features near that region - i.e. an eye near a nose near a mouth probably means a face - doesn't matter the position of the eye, it just matters that it's adjacent to the nose and the nose to the mouth. Similarly, a sharp pitch transition from one mel frequency band to another means we have an angry voice, doesn't matter if the transition happens at a certain point in time.  a 2x2 maxpool kernel keeps 1/4 values, so we reduce the number of parameters for that layer's output feature map by 75%.  In practice the inventor of maxpooling says \"it is almost a shame that it works, because we really don't know why\" (TWIML AI podcast episode ##) but it improves generalizability greatly. \n",
    "\n",
    "## Kernels and Filters\n",
    "kernels - kernels of the convolutional layer are in analogy to neurons in a fully-connected layer. Each kernel contains a number of weights equal to its size (i.e. depth x height x width). The kernel's size is a hyperparameter of the convolutional layer which is called its receptive field, since the kernel's size determines the dimensions of the input features it operates on. We take the products of the kernel's weights with the values of the input feature map and sum them to create a single entry in the output feature map. In this task, the first input feature map has values describing pixel brightness - in our case, we have 128x563 \"spectrogram pixels\", each pixel representing the intensity in decibels of each of 128 mel frequencies at each of 563 time points - in other words, we have a 128x563 grayscale image map with just one channel for brightness.  Each kernel constituting a convolutional layer's filter creates a single feature map - so more kernels = more feature maps. If our input feature map has 1 channel and 2D dimension H x W, and we want 16 output channels, we'll have 1 kernel in that layer's filter and produce 16 feature maps of H x W, or a single 16 x H x W feature map for the entire layer. In short, a higher output channel dimension creates a larger, more complicated output feature map. This may be beneficial or detrimental w.r.t. overfitting depending on the task. \n",
    "\n",
    "A 3x3 kernel with stride 1 on our input 128x563 will produce a 126x561 feature map.\n",
    "\n",
    "We can consider a collection of 2D kernels to create a 3D filter which defines a convolutional layer. For every input channel we have one 2D kernel, or for a multiple of input channels we have a collection of 2D kernels - i.e. a 3D filter. Each 2D kernel  creates a single contribution to one output channel - or the entire 3D filter creates one entire output channel. In the case where we expand the number of channels, i.e. expanding the complexity of the feature map in the channel/depth dimension while keeping constants it H x W dimensions. Each 3D filter is passed over the input feature map once for every complete output channel. To summarize, the number of inpuit channels determines the number of 2D kernels and thus, size of the entire layer's 3D filter, while the number of output channels determines how many filters are passed over the image - one unique 3D filter is required to create one unique output channel. Therefore, the input channel dimension determines the _size_ of the 3D filter in that convolutional layer, while the output channel dimension determines the _number_ of 3D filters in that layer. Each filter is defined by a unique set of weights, and each filter has its own bias term. In this way, we can think of each filter as a neuron of a convolutional layer. Each filter learns to represent a particular feature from the input feature map to the layer to which the filter belongs. Each filter creates a unique feature map. (In sum, the number of unique 2D kernels in a layer is equal to the number of input feature maps - the input channel dimension - and the number of unique 3D filters is equal to the number of output feature maps - the output channel dimension/ In sum, the number of input feature maps (input channel dimension) determines the number of unique 2D kernels, while the number of output feature maps (output channel dimension) determines the number of 3D filters.) This paradigm means that the convolutional layers actually perform a sort of feature extraction - they create high-order feature representations - and the classification is only done by subsequent non-convolutional layers, such as a fully connected linear layer.   Every layer of a ConvNet transforms the 3D input volume to a 3D output volume of neuron activations\n",
    "\n",
    " #typical maxpool kernel == stride == 2 - higher kernel throws away more informaiton and smaller kernel retains more, since it will take more steps over the input feature map. Some prominent networks (AlexNet, GoogleNet) use 3x3 kernels with stride length 2 for their maxpooling layers. I played around with maxpool layer combinations throughout the network of size 2x2, stride 2, up to 4x4, stride 4. The guiding principal was to try and find a decent balance of feature retention at each layer of the network. I found that a 2x2 non-overlapping maxpool kernel is necessary on the input layer to retain a decent balance of features going through subsequent convolutional layers, and thereafter a non-overlapping 4x4, stride 4 kernel - a 1/16 feature map reduction - provided an excellent computation boost on the 3 convolutional layers thereafter  with little trade-off in accuracy. Larger kernels on the convolutional layer usually provides better feature representations - more complex feature maps - as it provides (??)\n",
    " in contrast to fully connected layers, layers with maxpooling discard individual neruon's features and only retain the highest valued feature in that neuron's vicinity. \n",
    " \n",
    " maxpooling reduces the dimensions of each feature map output by each 3D filter of a convolutional layer. It's called 2D convoluton because the filters are only moved by their stride length in the H x W dimensions and encompass the entire depth of the feature map over which they operate at each step.  \n",
    "\n",
    "\n",
    "### More CNN Kernel/Filter Math: \n",
    "the overall filter for this convolutional layer will have 16 kernels (out_channels). We concatenate the feature map produced by each kernel to create feature map produced by the filter in the convolutional layer composed of those kernels: the (4320, 1,128,563) tensor becomes a (4320, 16, 128, 563) tensor which is fed into the next convolutional layer with a 32-kernel filter. Each kernel learns weights for the convolution it produces. \n",
    "\n",
    "1st block maps applies a 16-kernel filter to each sample in the batch, mapping the input features to a (16,128,563) feature map, maxpools to (16,32,141) then the next conv block applies a 32-kerne lfilter, mapping to   (16,32,141) map then (64,128,563) and the final layer preserves the size of the feature map. So in first layer we apply a 3x3 kernel to the 2D spectrogram image space, subsequent layers will apply 16 x 3x3 kernels, or a single 16x3x3 kernel - i.e. the (16,128,563) feature map is convolved with a (16,3,3) filter in that layer - however the stride remains in the h,w dimension, we take all the weights in the depth (16) dimension. We can think of each convolutional layer having out_channel (e.g. 16) number of in_channels x height x width kernels each with 3x3 weights, each of 16 1x3x3 kernels being applied to each of 16 1x128x563 input channel separately - after maxpooling, we get 16x128/4x563/4 (plus zero padding) filters. . In other words, the number of 2D filters matches the number of input channels. We can imagine the layer with 16 input channels has a filter with 16x3x3 weights which it applies to the 16x128x563 input feature map.\n",
    "\n",
    "We zero pad the input feature map at the start of each convolutional block, i.e. adding a column and row of zeroes to each side of the 2d minibatch (h,w) dimension. If a kernel's next stride will set it to be out of bounds of the last column of the 2d h,w dimension of the feature map, then it will proceed (take a step of length==stride) down to the next row. If our stride length is > 1, this means we will potentially lose features at the right and bottom edge of our feature map. Zero padding is standard in convolutional layers even with a stride of 1 - if we don't pad in this case, corner pixels (features) will only be used once. \n",
    "\n",
    "In the next convolutional block, extendidng exactly the above, since we input 16 channels we have a 16x3x3 filter on that layer produing 32 output channels - so we have 32 16x3x3 filters, each filter composed of 16 kernels each with their own weights (or 16x3x3 weights for the filter), x32 for the entire convolutional layer. So we apply 32 differently weighted 16x3x3 filters to the 16x32x141 (h, w reduced 75% by maxpooling, depth is kept)  input feature map to produce a 32x32x141 feature map which after maxpooling becomes 32x8x35\n",
    "\n",
    "each convolutional layer has n_output 3-dimensional filters of size n_input x h x w to account for the n_output output channels where each has a value for the depth dimension of n_input to match the # of input channels. In total there are\n",
    "\n",
    "number_of_filters=input_channel*output_channels=1*16=16, then 16*32 = 512, then 32*64 = 2048 2D filters each of size==kernel size^2 in each convolutional layer \n",
    "\n",
    "If the feature that a 3D filter has learned is present inthe feature map over which it operates, it will produce a higher magnitude feature map relative to filters which have not found in the input the features they have learned. After ReLU, A filter which has recognized an image produces a higher magnitude activation at a given location of its output volume. The filters in each convolutional layer are likely to learn a different set of high-level features, in a hierarchical manner of increasing complexity with each subsequent convolutional layer.\n",
    "\n",
    "### Dropout and Pruning\n",
    "With dropout, we simply deactivate a number of neurons from each layer of our network, each neuron having a probability p to be deactivated 1-p to be kept. In this network we implement dropout of 30% of features at the end of each 2D convolutional layer after maxpooling the relu transformed outputs.  [the OG read on dropout](https://arxiv.org/abs/1207.0580). The resulting activations are scaled by 1/(1-p) so that the weights are appropriately scaled i.e. not inflated for those activations that are kept so the same network with no dropout is used on test data. so we keep the sum of the weights the same when using dropouts,  so we produce the same expected value of activations  independent of dropout probability - otherwise the activaitons from the kept neurons would cause incorrect weights to be learned. Anyway, dropout is preetty simple and applicable to a wide variety of neural network architectures. Think of it as kind of a \"weight loss\" for your network, pun intended - making the network faster - during training. **We turn off dropout (i.e. we keep all learned neurons) for validation and training.** \n",
    "\n",
    "If we were to turn off certain neurons during validation and training, that would no longer be considered dropout - that would be pruning, where we look for and remove all connections with \"dead\" neurons - those with little contribution or otherwise undesirable contributions to the network. When we perform pruning on a network, we would be removing certain clusters of neurons - feature representations, i.e. filters from a CNN. There's been some recent progress in the field of CNN pruning with some pretty exciting results - for example, a [60% reduction in network parameters with a 0.6% accuracy loss](https://arxiv.org/abs/2001.08142)\n",
    "\n",
    "### Batch Normalization, Internal Covariate Shift, and Optimizing the Optimization Surface \n",
    "each layers net input distribution changes as the network is trained because the previous layer's output changes when its parameters are adjusted. non saturating nonlinearity is highly affected by weights of higher magnitude while saturating nonlinearities may make a model slower to train (beacuse large weights will be squshed?) - because the differential of a saturating non-linearity activation for outputs beyond +-1 approaches 0, the error differential between such outputs also approaches zero meaning no change to backpropogation for such outputs - i.e. we encounter a variation of the vanishing gradient problem. batch normalizing re-centers or re-normalizes the data so to speak so features always belong to similar distributions. \n",
    "\n",
    "because of these changing net input distributions, the loss surface of the optimizer becomes more volatile as its loss surface is defined for  setting of the weights which will change in unpredictable ways on unstable net inputs - i.e. the loss manifold . As such,  lower learning rates are needed to ensure weights are moved in the direction of a more stable negative gradient, otherwise it is more likely to converge on suboptimal solutions before proper model parameters are learned. the parameter initialization is also important to consider in avoiding suboptimal convergence because it so. \n",
    "\n",
    "internal covariate shift is change in the distributon of network outputs (activations) due to the change in network parameters i.e. the shift in the output and parameter covariates. as output of each layer changes, the distribution of net inputs to all subsequent layers change - so the difficulty for each layer becomes attempting to model a changing distibution of net inputs. batch normalization attempts to make the net input distribution to each hidden layer similar throughout training. e.g. covariate shift:  input features sacled to have zero mean and unit variance, but once being output as activations of the subsequent hidden layer different features sets may now belong to different distributions\n",
    "\n",
    "the vanishing gradient problem is an issue with the saturating nonlinear sigmoid and tanh activation functions (outputs beyond +-1 are saturated) - batch normalization encourages each layer's neurons to produce outputs closer to the linear regions of their activationt, i.e. attempts to center the activation from the sigmoid function (to 0.5) and tanh function (to 0.)\n",
    "\n",
    " batch normalization is a technique to mitigate internal covariate shift. batch normlaization stabilizes the differential of distributions of internal (intermediate) activations, and this leads to faster convergence - BN steers the model away from becoming stuck in a saturated learning regime with a vanishing gradient. this normalization is implemented internally as an aspect of the model's architechture, so the output from each training minibatch is normalized at each layer. Since this mitigates the issue of internal covariate shift, the network can be initialized with a broader range of parameters and a higher learning rate and still achieve good accuracy. Batch normalization also regularizes learned coefficients in this way, discouraging  higher coefficients present in overfit models. The authors of the paper show by observation that batch normalization works, altghough they don't provide proof as to why.  The paper shows this empirically instead of with a mathematical proof - which is perhaps why some disagree with the reasoning behind BN:\n",
    "\n",
    "However, in [this paper](https://arxiv.org/pdf/1805.11604.pdf#:~:text=Batch%20Normalization%20(BatchNorm)%20is%20a,deep%20neural%20networks%20(DNNs).&text=This%20smoothness%20induces%20a%20more,gradients%2C%20allowing%20for%20faster%20training.) the authors argue that the stabilziation of the internal covariate shift (changing input distrubtions) is not the main factor behind BN's success, but rather it is smoothing of the optimizer's loss function landscape. a smoother loss surface creates a more stable direction of the gradient differential between each training step i.e. moving towards a good minima and the gradients are more predictable leading to faster training since we can use higher learning rates when the gradient moves predictably in the direction of a global minima, otherwise we would need frequent recalculation of the gradient in a region to make sure we move in the right direciton. \n",
    "\n",
    "In short, the default behaviour of batch norm and as it is implemented in pytorch is essentially applying standard scaling (subtract mean, divide by standard deviation) to the tensor's 2D minibatch dimension. (i.e. dim=2 and dim=3 in the N x C x H x W tensor format).  \n",
    "\n",
    "By the way, we turn off BN when we validate and test a model - it's just a technique to improve and accelerate training. As far as I know, we don't use BN for validation and testing of a model.\n",
    "\n",
    "### ReLU for Activation or, Non-Saturated Activations are Healthier (for CNNs)\n",
    "We're also using ReLU for activation, as is standard in CNN to avoid saturation of weights - large weights aren't squashed and we  further discourage a vanishing gradient. This model would take much longer to train with tanh/sigmoid with negligible performance increase. Especially important due to the moderate depth of this model, because with a saturating nonlinear activaiton we would have increasing difficulty trainig subsequent layers. We want a decent amount of layers to learn a higher representation of features, e.g. not just frequency transitions but the shape and magnitude of these transitions with respect to one another. gradient is usually high if a neuron is activated with ReLU, producing effective successive gradients in the network. Sigmoid and tanh produce near-zero gradients for very high magnitude inputs. relu is much cheaper han tanh or sigmoid because it computes max(0,input) vs the exponential in other nonlinear functions. All in all, ReLU will lead to much faster convergence and for this task produces excellent results. \n",
    "\n",
    "### More About the Optimization Surface\n",
    "other methods for taming optimization surface include skip connections, where a layer's outputs skip some of its subsequent layers - so implemented by the extremely successful ResNet (residiual network) architechture - taking inspiration from certain cells in the cerebral cortex. Such skip connections require a weight matrix of their own to be effective if skipping over more than one nonlinear layer. local minima worse htan the global minima become shallower, i.e. their depth decreases and this eliminates \"bad\" local minima in the loss landsacpe. \n",
    "(image of loss landsacpe with citation, difference with skip weights)\n",
    "the loss landsacpe is formed by the function L(weights) = avg of sum i to m training samples over L(xi, yi, weights) - (the surface is formed by each xi, yi, weight - however due to the high parameterization of neural networks the visual representation of the loss landscape is very high dimensional and not as simple, although the difference can be normalized to provide an intuitive understanding of loss as in the image.\n",
    "\n",
    "depth of the loss function - practically, the degree of change coefficients will undergo due to the loss function's behaviour - allows more flexibility in a model and for CNNs, ultimately the ability to learn higher level features built on outputs of previous layers,  but at the same time encourages overfitting to noise. In contrast, width smoothes the optimization landscape and eliminates many bad local minima\n",
    "\n",
    "\n",
    "\n",
    "## Turning Feature Maps into Probabilities with a Fully Connected Layer\n",
    "After our last convolutional layer produces its output feature maps - # output channels 2D feature maps, or one 3D feature map. since regular hidden layers operate only on 1D arrays, the entire feature map is flattened to a 1D array so it can be connected to all input neurons in the next fully connected layer, thusly termed - . In this network, we have an output \n",
    "\n",
    "using fully connected layer at the end of convolution as a standard architechture - takes the features with positions relative to eachother and processes them together. I.e. we have all these sets of mel frequency transitions in one feature map - the voice is angry. The FC layer operates on the combined feature map (a tensor) from the combined by the conv and transformer embeddings.  \n",
    "\n",
    "we have a parallel model where we pass the mel spec through a 4 layer transformer-encoder and 4 block 2d convolutional network at the same time to create activation tensors which we concatenate to form a single tensor which is then passed to the FC linear layer converting 320 features for all samples to 8 activations. \n",
    "\n",
    "the FC layer produces 8 activations (unnormalized logits) based on the activation maps/tensors produced by the filter of each convolutional layer. We need the  FC layer to combine higher-level features learnt by the end result of the convolution filters so that the network can build a global representation about the spectrogram - high level features may be present across different classes, but a specific combinaiton of these features more confidently represents an emotion. Another way to see this is the FC layer evaluates which of the high order features learned by the convolution filters is present in the current sample. \n",
    "\n",
    "each 2d block is like a LeNet architechture: Conv->relu>pool. The original 2015 BN paper suggests that \"We add the BN transform immediately before the nonlinearity\" i.e. before ReLU, but I found that I got better performance out of this architechture using BN after ReLU. **See Keras author's [Francois Chollet's response on GitHub](https://github.com/keras-team/keras/issues/1802#issuecomment-187966878) regarding the BN order issue: \"I can guarantee that recent code written by Christian \\[Szegedy\\] but I can guarantee that recent code written by Christian applies relu before BN\".** \n",
    "\n",
    "\n",
    "\n",
    "# architechture\n",
    "VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper Very Deep Convolutional Networks for Large-Scale Image Recognition. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC-2014. It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 33 kernel-sized filters one after another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPUs.\n",
    "\n",
    "The input to cov1 layer is of fixed size 224 x 224 RGB image. The image is passed through a stack of convolutional (conv.) layers, where the filters were used with a very small receptive field: 33 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations, it also utilizes 11 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1-pixel for 33 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv.  layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 22 pixel window, with stride 2.\n",
    "\n",
    "https://arxiv.org/pdf/1409.1556.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### note to self: each output of the convolution layer is a feature map, e.g. input 1x128x563 5feature map, then 16x32x141 feature map etc.  - reword accordingly\n",
    "# draw it : https://github.com/gwding/draw_convnet\n",
    "https://github.com/HarisIqbal88/PlotNeuralNet\n",
    "http://alexlenail.me/NN-SVG/LeNet.html\n",
    "### each stack of squares in each convolutional layer is a feature map, or the result of a unique 3D filter. Each square represents a unique higher-order feature. Each square in itself is composed of a number of unique 2D kernels. \n",
    "\n",
    "\n",
    "# CREATE DIFFERENT FORWARD PASS FUNCTIONS FOR THE ARCHITECHTURES\n",
    "### something something CNN-LSTM is a flagship of image processing and applies exceptionally well to audio classification since it combines the specotrgram image processing powers of a CNN and the time-sequence modelling properties of the LSTM.\n",
    "## time distributed cnn is especially suited to audio data \n",
    "### mabe avoid time distributed for now and implement the GANs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model Architecture and Define Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change nn.sequential to take dict to make more readable \n",
    "\n",
    "class ParallelModel(nn.Module):\n",
    "    # Define all layers present in the network\n",
    "    def __init__(self,num_emotions):\n",
    "        super().__init__() \n",
    "        \n",
    "        ############### 1ST PARALLEL CONVOLUTIONAL BLOCK ############\n",
    "        # 3 sequential conv2D layers: (1,128,563) --> (16, 64, 281) -> (32, 8, 35) -> (64, 1, 4)\n",
    "        self.conv2Dblock1 = nn.Sequential(\n",
    "            \n",
    "            # 1st 2D convolution layer\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, # input volume depth == input channel dim == 1\n",
    "                out_channels=16, # expand output feature map volume's depth to 16\n",
    "                kernel_size=3, # typical 3*3 stride 1 kernel\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
    "            nn.ReLU(), # feature map --> activation map\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
    "            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
    "            \n",
    "            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
    "            nn.Conv2d(\n",
    "                in_channels=16, \n",
    "                out_channels=32, # expand output feature map volume's depth to 32\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=8, stride=8), # increase maxpool kernel for subsequent filters\n",
    "            nn.Dropout(p=0.3), \n",
    "            \n",
    "            # 3rd 2D convolution layer identical to last except output dim\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64, # expand output feature map volume's depth to 64\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=8, stride=8),\n",
    "            nn.Dropout(p=0.3),\n",
    "        )\n",
    "        \n",
    "        ############### 2ND PARALLEL CONVOLUTIONAL BLOCK ############\n",
    "        # 3 sequential conv2D layers (1,128,563) --> (16, 64, 281) -> (32, 8, 35) -> (64, 1, 4)\n",
    "        self.conv2Dblock2 = nn.Sequential(\n",
    "\n",
    "            # 1st 2D convolution layer\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, # input volume depth == input channel dim == 1\n",
    "                out_channels=16, # expand output feature map volume's depth to 16\n",
    "                kernel_size=3, # typical 3*3 stride 1 kernel\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
    "            nn.ReLU(), # feature map --> activation map\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
    "            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
    "\n",
    "            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
    "            nn.Conv2d(\n",
    "                in_channels=16, \n",
    "                out_channels=32, # expand output feature map volume's depth to 32\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=8, stride=8), # increase maxpool kernel for subsequent filters\n",
    "            nn.Dropout(p=0.3), \n",
    "\n",
    "            # 3rd 2D convolution layer identical to last except output dim\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64, # expand output feature map volume's depth to 64\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=8, stride=8),\n",
    "            nn.Dropout(p=0.3),\n",
    "        )\n",
    "        \n",
    "        ################ TRANSFORMER BLOCK #############################\n",
    "        # maxpool the input feature map/tensor to the transformer \n",
    "        # a rectangular kernel worked better here for the rectangular input spectrogram feature map/tensor\n",
    "        self.transformer_maxpool = nn.MaxPool2d(kernel_size=[2,4], stride=[2,4])\n",
    "        \n",
    "        # define single transformer encoder layer\n",
    "        # self-attention + feedforward network from \"Attention is All You Need\" paper\n",
    "        # 4 multi-head self-attention layers each with 64-->512--->64 feedforward network\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=64, # input feature (frequency) dim after maxpooling 128*563 -> 64*140 (freq*time)\n",
    "            nhead=4, # 4 multihead self-attention layers in each encoder block\n",
    "            dim_feedforward=512, # 2 linear layers in each encoder block's feedforward network: dim 64-->512--->64\n",
    "            dropout=0.4, \n",
    "            activation='relu' # ReLU: avoid saturation/tame gradient/reduce compute time\n",
    "        )\n",
    "        \n",
    "        # I'm using 4 instead of the 6 identical stacked encoder layrs used in Attention is All You Need paper\n",
    "        # Complete transformer block contains 4 full transformer encoder layers (each w/ multihead self-attention+feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)\n",
    "        \n",
    "        # Linear softmax layer to take final concatenated embedding tensor \n",
    "        #    from parallel 2D convolutional and transformer blocks, output 8 logits \n",
    "        # Each full convolution block outputs (64*1*4) embedding flattened to dim 256 1D array \n",
    "        # Full transformer block outputs 64*140 feature map, then we time-avg to dim 64 1D array\n",
    "        # 256*2+64 == 576 input features --> 8 output emotions \n",
    "        self.fc1_linear = nn.Linear(256*2 + 64,num_emotions) \n",
    "        \n",
    "        # Softmax layer for the 8 output logits from final FC linear layer \n",
    "        self.softmax_out = nn.Softmax(dim=1) # dim==1 is the freq embedding\n",
    "        \n",
    "    # define one complete parallel forward pass of input feature map/tensor through conv+transformer layers\n",
    "    def forward(self,x):\n",
    "        \n",
    "        ############ 1st parallel Conv2D block: 4 Convolutional layers ############################\n",
    "        # create final feature embedding from 1st convolutional layer \n",
    "        # input features pased through 4 sequential 2D convolutional layers\n",
    "        conv2d_embedding1 = self.conv2Dblock(x) # x == N/batch * channel * freq * time\n",
    "        \n",
    "        # flatten final 64*1*4 feature map from convolutional layers to length 256 1D array \n",
    "        # skip the 1st (N/batch) dimension when flattening\n",
    "        conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim=1) \n",
    "        \n",
    "        ############ 2nd parallel Conv2D block: 4 Convolutional layers #############################\n",
    "        # create final feature embedding from 2nd convolutional layer \n",
    "        # input features pased through 4 sequential 2D convolutional layers\n",
    "        conv2d_embedding2 = self.conv2Dblock(x) # x == N/batch * channel * freq * time\n",
    "        \n",
    "        # flatten final 64*1*4 feature map from convolutional layers to length 256 1D array \n",
    "        # skip the 1st (N/batch) dimension when flattening\n",
    "        conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim=1) \n",
    "        \n",
    "        \n",
    "        ########## 4-encoder-layer Transformer block w/ 64-->512-->64 feedfwd network ##############\n",
    "        # maxpool input feature map: 1*128*563 w/ 2*4 kernel --> 1*64*140\n",
    "        x_maxpool = self.transformer_maxpool(x)\n",
    "        # remove channel dim: 1*64*140 --> 64*140\n",
    "        x_maxpool_reduced = torch.squeeze(x_maxpool,1)\n",
    "        \n",
    "        # convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format\n",
    "        # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)\n",
    "        x = x_maxpool_reduced.permute(2,0,1) \n",
    "        \n",
    "        # finally, pass reduced input feature map x into transformer encoder layers\n",
    "        transformer_output = self.transformer_encoder(x)\n",
    "        \n",
    "        # create final feature emedding from transformer layer by taking mean in the time dimension (now the 0th dim)\n",
    "        # transformer outputs 64*140 (freq embedding*time) feature map, take mean of all columns i.e. take time average\n",
    "        transformer_embedding = torch.mean(transformer_output, dim=0) # dim 64*140 --> 64\n",
    "        \n",
    "        ############# concatenate freq embeddings from convolutional and transformer blocks ######\n",
    "        # concatenate embedding tensors output by parallel 2*conv and 1*transformer blocks\n",
    "        complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2, transformer_embedding], dim=1)  \n",
    "       \n",
    "        ######### final FC linear layer, need logits for loss #########################\n",
    "        output_logits = self.fc1_linear(complete_embedding)  \n",
    "        \n",
    "        ######### Final Softmax layer: use logits from FC linear, get softmax for prediction ######\n",
    "        output_softmax = self.softmax_out(output_logits)\n",
    "        \n",
    "        # need output logits to compute cross entropy loss, need softmax probabilities to predict class\n",
    "        return output_logits, output_softmax\n",
    "                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing The Flow of Tensors Through the Network\n",
    "We zero-pad 1 the input feature map to each convolutional layer to get back from the layer the same shape tensor as we input: zero-pad 1 adds 2 to each of (H, W) dims, and the 3x3, stride 1 kernal's cuts off (kernel - stride == 2) dims from each of (H,W) which effectively throws away the zero pads. \n",
    "\n",
    "At the end of each convolutional layer we have a maxpool kernel of size 2x2, stride 2 which will take 1 of 4 pixels in its winddow. For the input feature map the maxpool kernel will progress 128/2 = 64 times over the rows and 563/2=281 times over the columns making an output feature map of 64*281. \n",
    "\n",
    "Here's the complete flow through the network in Sample (Batch) x Channel x Height x Width format:\n",
    "NxCxHxW ----Layer 1----> IN: 1x128x563 -> PAD-1: 130x565 -> OUT 16x128x563 -> MAXPOOL 2x2: 16x64x281 ----Layer 2------> in 16*64*281 --> pad 1 16*66*283 ----> out 32*64*281 -> maxpool 32*16*70--------> in 32*16*70 ---> pad1  32*18*72 --> out 64*16*70 --> maxpool 64*4*17 -----> in 64*4*17 -> pad 1 64*6*19 --> out 64*4*17 --> maxpool 64*1*4 -> flatten 64*1*4 = 256. \n",
    "\n",
    "Our tensor's flow through the network is shown as the input/output of each successive convolutional block after maxpooling:\n",
    "Convolutional block 1: N, 1, 128, 563 -> N, 16, 64, 281 \n",
    "Convolutional block 2: N, 16, 64, 281 -> N, 32, 16, 70\n",
    "Convolutional block 3: N, 32, 16, 70 -> N, 64, 4, 17\n",
    "Convolutional block 4: N, 64, 4, 17 -> N, 64, 1, 4\n",
    "flatten convolutional embedding: N, 64, 1, 4 -> N, 256\n",
    "FINAL CONVOLUTIONAL EMBEDDING: N, 256\n",
    "\n",
    "flow through transformer layers:\n",
    "Maxpool input feature map w/ 2x4 kernel : N, 1, 128, 563 ---> N, 1, 64, 140\n",
    "drop channel and permute: N, 1, 64, 140 --> N, 64, 140 --> 140, N, 64\n",
    "transformer encoder block: 140, N, 64 --> 140, N, 64\n",
    "time average transformer embedding: 140, N, 64 -> N, 64 \n",
    "FINAL TRANSFORMER EMBEDDING: N, 64\n",
    "\n",
    "Concatenate to N, 320 \n",
    "Linear Layer: N, 320 ---> N, 8\n",
    "Softmax layer: N, 8 ----> N, 1 where 1 is predicted emotion for each of N samples \n",
    "\n",
    "We can confirm our network's tensor shapes and flow using the excellent torchsummary package which provides a pytorch implements of Keras' model.summary method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `needed` not found.\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 128, 563]             160\n",
      "       BatchNorm2d-2         [-1, 16, 128, 563]              32\n",
      "              ReLU-3         [-1, 16, 128, 563]               0\n",
      "         MaxPool2d-4          [-1, 16, 64, 281]               0\n",
      "           Dropout-5          [-1, 16, 64, 281]               0\n",
      "            Conv2d-6          [-1, 32, 64, 281]           4,640\n",
      "       BatchNorm2d-7          [-1, 32, 64, 281]              64\n",
      "              ReLU-8          [-1, 32, 64, 281]               0\n",
      "         MaxPool2d-9           [-1, 32, 16, 70]               0\n",
      "          Dropout-10           [-1, 32, 16, 70]               0\n",
      "           Conv2d-11           [-1, 64, 16, 70]          18,496\n",
      "      BatchNorm2d-12           [-1, 64, 16, 70]             128\n",
      "             ReLU-13           [-1, 64, 16, 70]               0\n",
      "        MaxPool2d-14            [-1, 64, 4, 17]               0\n",
      "          Dropout-15            [-1, 64, 4, 17]               0\n",
      "           Conv2d-16            [-1, 64, 4, 17]          36,928\n",
      "      BatchNorm2d-17            [-1, 64, 4, 17]             128\n",
      "             ReLU-18            [-1, 64, 4, 17]               0\n",
      "        MaxPool2d-19             [-1, 64, 1, 4]               0\n",
      "          Dropout-20             [-1, 64, 1, 4]               0\n",
      "        MaxPool2d-21           [-1, 1, 64, 140]               0\n",
      "MultiheadAttention-22  [[-1, 2, 64], [-1, 140, 140]]               0\n",
      "          Dropout-23                [-1, 2, 64]               0\n",
      "        LayerNorm-24                [-1, 2, 64]             128\n",
      "           Linear-25               [-1, 2, 512]          33,280\n",
      "          Dropout-26               [-1, 2, 512]               0\n",
      "           Linear-27                [-1, 2, 64]          32,832\n",
      "          Dropout-28                [-1, 2, 64]               0\n",
      "        LayerNorm-29                [-1, 2, 64]             128\n",
      "TransformerEncoderLayer-30                [-1, 2, 64]               0\n",
      "MultiheadAttention-31  [[-1, 2, 64], [-1, 140, 140]]               0\n",
      "          Dropout-32                [-1, 2, 64]               0\n",
      "        LayerNorm-33                [-1, 2, 64]             128\n",
      "           Linear-34               [-1, 2, 512]          33,280\n",
      "          Dropout-35               [-1, 2, 512]               0\n",
      "           Linear-36                [-1, 2, 64]          32,832\n",
      "          Dropout-37                [-1, 2, 64]               0\n",
      "        LayerNorm-38                [-1, 2, 64]             128\n",
      "TransformerEncoderLayer-39                [-1, 2, 64]               0\n",
      "MultiheadAttention-40  [[-1, 2, 64], [-1, 140, 140]]               0\n",
      "          Dropout-41                [-1, 2, 64]               0\n",
      "        LayerNorm-42                [-1, 2, 64]             128\n",
      "           Linear-43               [-1, 2, 512]          33,280\n",
      "          Dropout-44               [-1, 2, 512]               0\n",
      "           Linear-45                [-1, 2, 64]          32,832\n",
      "          Dropout-46                [-1, 2, 64]               0\n",
      "        LayerNorm-47                [-1, 2, 64]             128\n",
      "TransformerEncoderLayer-48                [-1, 2, 64]               0\n",
      "MultiheadAttention-49  [[-1, 2, 64], [-1, 140, 140]]               0\n",
      "          Dropout-50                [-1, 2, 64]               0\n",
      "        LayerNorm-51                [-1, 2, 64]             128\n",
      "           Linear-52               [-1, 2, 512]          33,280\n",
      "          Dropout-53               [-1, 2, 512]               0\n",
      "           Linear-54                [-1, 2, 64]          32,832\n",
      "          Dropout-55                [-1, 2, 64]               0\n",
      "        LayerNorm-56                [-1, 2, 64]             128\n",
      "TransformerEncoderLayer-57                [-1, 2, 64]               0\n",
      "TransformerEncoder-58                [-1, 2, 64]               0\n",
      "           Linear-59                    [-1, 8]           2,568\n",
      "          Softmax-60                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 328,616\n",
      "Trainable params: 328,616\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.27\n",
      "Forward/backward pass size (MB): 30.10\n",
      "Params size (MB): 1.25\n",
      "Estimated Total Size (MB): 31.63\n",
      "----------------------------------------------------------------\n",
      "Object `needed` not found.\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 128, 563]             160\n",
      "       BatchNorm2d-2         [-1, 16, 128, 563]              32\n",
      "              ReLU-3         [-1, 16, 128, 563]               0\n",
      "         MaxPool2d-4          [-1, 16, 64, 281]               0\n",
      "           Dropout-5          [-1, 16, 64, 281]               0\n",
      "            Conv2d-6          [-1, 32, 64, 281]           4,640\n",
      "       BatchNorm2d-7          [-1, 32, 64, 281]              64\n",
      "              ReLU-8          [-1, 32, 64, 281]               0\n",
      "         MaxPool2d-9           [-1, 32, 16, 70]               0\n",
      "          Dropout-10           [-1, 32, 16, 70]               0\n",
      "           Conv2d-11           [-1, 64, 16, 70]          18,496\n",
      "      BatchNorm2d-12           [-1, 64, 16, 70]             128\n",
      "             ReLU-13           [-1, 64, 16, 70]               0\n",
      "        MaxPool2d-14            [-1, 64, 4, 17]               0\n",
      "          Dropout-15            [-1, 64, 4, 17]               0\n",
      "           Conv2d-16            [-1, 64, 4, 17]          36,928\n",
      "      BatchNorm2d-17            [-1, 64, 4, 17]             128\n",
      "             ReLU-18            [-1, 64, 4, 17]               0\n",
      "        MaxPool2d-19             [-1, 64, 1, 4]               0\n",
      "          Dropout-20             [-1, 64, 1, 4]               0\n",
      "        MaxPool2d-21           [-1, 1, 64, 140]               0\n",
      "MultiheadAttention-22  [[-1, 2, 64], [-1, 140, 140]]               0\n",
      "          Dropout-23                [-1, 2, 64]               0\n",
      "        LayerNorm-24                [-1, 2, 64]             128\n",
      "           Linear-25               [-1, 2, 512]          33,280\n",
      "          Dropout-26               [-1, 2, 512]               0\n",
      "           Linear-27                [-1, 2, 64]          32,832\n",
      "          Dropout-28                [-1, 2, 64]               0\n",
      "        LayerNorm-29                [-1, 2, 64]             128\n",
      "TransformerEncoderLayer-30                [-1, 2, 64]               0\n",
      "MultiheadAttention-31  [[-1, 2, 64], [-1, 140, 140]]               0\n",
      "          Dropout-32                [-1, 2, 64]               0\n",
      "        LayerNorm-33                [-1, 2, 64]             128\n",
      "           Linear-34               [-1, 2, 512]          33,280\n",
      "          Dropout-35               [-1, 2, 512]               0\n",
      "           Linear-36                [-1, 2, 64]          32,832\n",
      "          Dropout-37                [-1, 2, 64]               0\n",
      "        LayerNorm-38                [-1, 2, 64]             128\n",
      "TransformerEncoderLayer-39                [-1, 2, 64]               0\n",
      "MultiheadAttention-40  [[-1, 2, 64], [-1, 140, 140]]               0\n",
      "          Dropout-41                [-1, 2, 64]               0\n",
      "        LayerNorm-42                [-1, 2, 64]             128\n",
      "           Linear-43               [-1, 2, 512]          33,280\n",
      "          Dropout-44               [-1, 2, 512]               0\n",
      "           Linear-45                [-1, 2, 64]          32,832\n",
      "          Dropout-46                [-1, 2, 64]               0\n",
      "        LayerNorm-47                [-1, 2, 64]             128\n",
      "TransformerEncoderLayer-48                [-1, 2, 64]               0\n",
      "MultiheadAttention-49  [[-1, 2, 64], [-1, 140, 140]]               0\n",
      "          Dropout-50                [-1, 2, 64]               0\n",
      "        LayerNorm-51                [-1, 2, 64]             128\n",
      "           Linear-52               [-1, 2, 512]          33,280\n",
      "          Dropout-53               [-1, 2, 512]               0\n",
      "           Linear-54                [-1, 2, 64]          32,832\n",
      "          Dropout-55                [-1, 2, 64]               0\n",
      "        LayerNorm-56                [-1, 2, 64]             128\n",
      "TransformerEncoderLayer-57                [-1, 2, 64]               0\n",
      "TransformerEncoder-58                [-1, 2, 64]               0\n",
      "           Linear-59                    [-1, 8]           2,568\n",
      "          Softmax-60                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 328,616\n",
      "Trainable params: 328,616\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.27\n",
      "Forward/backward pass size (MB): 30.10\n",
      "Params size (MB): 1.25\n",
      "Estimated Total Size (MB): 31.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# instantiate model for 8 emotions\n",
    "model = ParallelModel(8).to(device)\n",
    "\n",
    "# include input feature map dims in call to summary()\n",
    "summary(model, input_size=(1,128,563))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define criterion, i.e. loss function used calculate error for backwards pass of each training iteration\n",
    "# since our classes our balanced we don't need to specify a class-weight parameter (to balance classes)\n",
    "# pytorch's nn.CrossEntropyLoss() implements  log softmax and negative log likelihood loss (nn.NLLoss() --> nn.LogSoftmax())\n",
    "# we use log softmax for computation benefits and faster gradient optimization (harsher penalization of incorrect predictions) \n",
    "def criterion(predictions, targets): \n",
    "    return nn.CrossEntropyLoss(input=predictions,target=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to create a single step of the training phase\n",
    "def make_train_step(model, criterion, optimizer):\n",
    "    # define the training step of the training phase\n",
    "    def train_step(X,Y):\n",
    "        # forward pass\n",
    "        output_logits, output_softmax = model(X)\n",
    "        predictions = torch.argmax(output_softmax,dim=1)\n",
    "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "        \n",
    "        # compute loss on logits because nn.CrossEntropyLoss implements log softmax\n",
    "        loss = criterion(output_logits, Y) \n",
    "        \n",
    "        # compute gradients for the optimizer to use \n",
    "        loss.backward()\n",
    "        \n",
    "        # update network parameters based on gradient stored (by calling loss.backward())\n",
    "        optimizer.step()\n",
    "        \n",
    "        # zero out gradients for next pass\n",
    "        # pytorch accumulates gradients from backwards passes (convenient for RNNs)\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        return loss.item(), accuracy*100\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_validate_fnc(model,criterion):\n",
    "    def validate(X,Y):\n",
    "        # don't want to update any network parameters on validation passes: don't need gradient\n",
    "        # wrap in torch.no_grad to save memory and compute in validation phase: \n",
    "        with torch.no_grad(): \n",
    "            \n",
    "            # set model to validation phase i.e. turn off dropout and batchnorm layers \n",
    "            model.eval()\n",
    "            \n",
    "            # get the model's predictions on the validation set\n",
    "            output_logits, output_softmax = model(X)\n",
    "            predictions = torch.argmax(output_softmax,dim=1)\n",
    "            \n",
    "            # calculate the mean accuracy over the entire validation set\n",
    "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "            \n",
    "            # compute error from logits (nn.crossentropy implements softmax)\n",
    "            loss = criterion(output_logits,Y)\n",
    "            \n",
    "        return loss.item(), accuracy*100, predictions\n",
    "    return validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stack data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device is cuda\n",
      "Number of trainable params:  395176\n",
      " Epoch 0: iteration 106/107\n",
      "Epoch 0 --> loss:2.0801, acc:24.38%, val_loss:1.9979, val_acc:31.72%\n",
      " Epoch 1: iteration 106/107\n",
      "Epoch 1 --> loss:1.7457, acc:33.59%, val_loss:1.7944, val_acc:38.62%\n",
      " Epoch 2: iteration 106/107\n",
      "Epoch 2 --> loss:1.6374, acc:37.36%, val_loss:1.9507, val_acc:33.10%\n",
      " Epoch 3: iteration 106/107\n",
      "Epoch 3 --> loss:1.5793, acc:39.83%, val_loss:1.7433, val_acc:39.31%\n",
      " Epoch 4: iteration 106/107\n",
      "Epoch 4 --> loss:1.4730, acc:43.42%, val_loss:1.5088, val_acc:45.75%\n",
      " Epoch 5: iteration 106/107\n",
      "Epoch 5 --> loss:1.4228, acc:45.30%, val_loss:1.4489, val_acc:48.97%\n",
      " Epoch 6: iteration 106/107\n",
      "Epoch 6 --> loss:1.3656, acc:48.67%, val_loss:1.5508, val_acc:41.84%\n",
      " Epoch 7: iteration 106/107\n",
      "Epoch 7 --> loss:1.3094, acc:50.72%, val_loss:1.3872, val_acc:51.03%\n",
      " Epoch 8: iteration 106/107\n",
      "Epoch 8 --> loss:1.2850, acc:51.01%, val_loss:1.4482, val_acc:49.66%\n",
      " Epoch 9: iteration 106/107\n",
      "Epoch 9 --> loss:1.1830, acc:54.93%, val_loss:1.3544, val_acc:52.18%\n",
      " Epoch 10: iteration 106/107\n",
      "Epoch 10 --> loss:1.1633, acc:56.43%, val_loss:1.2083, val_acc:56.09%\n",
      " Epoch 11: iteration 106/107\n",
      "Epoch 11 --> loss:1.1356, acc:57.51%, val_loss:1.2495, val_acc:56.32%\n",
      " Epoch 12: iteration 106/107\n",
      "Epoch 12 --> loss:1.0524, acc:59.42%, val_loss:1.2644, val_acc:56.09%\n",
      " Epoch 13: iteration 106/107\n",
      "Epoch 13 --> loss:1.0259, acc:61.36%, val_loss:1.1268, val_acc:56.78%\n",
      " Epoch 14: iteration 106/107\n",
      "Epoch 14 --> loss:0.9697, acc:64.52%, val_loss:1.1491, val_acc:62.07%\n",
      " Epoch 15: iteration 106/107\n",
      "Epoch 15 --> loss:0.9325, acc:65.77%, val_loss:1.2621, val_acc:54.94%\n",
      " Epoch 16: iteration 106/107\n",
      "Epoch 16 --> loss:0.9373, acc:66.35%, val_loss:1.0800, val_acc:58.16%\n",
      " Epoch 17: iteration 106/107\n",
      "Epoch 17 --> loss:0.8724, acc:67.42%, val_loss:1.2557, val_acc:57.47%\n",
      " Epoch 18: iteration 106/107\n",
      "Epoch 18 --> loss:0.7846, acc:70.52%, val_loss:1.1066, val_acc:61.84%\n",
      " Epoch 19: iteration 106/107\n",
      "Epoch 19 --> loss:0.8003, acc:70.14%, val_loss:1.0165, val_acc:65.52%\n",
      " Epoch 20: iteration 106/107\n",
      "Epoch 20 --> loss:0.7231, acc:72.03%, val_loss:0.8726, val_acc:67.36%\n",
      " Epoch 21: iteration 106/107\n",
      "Epoch 21 --> loss:0.6803, acc:74.81%, val_loss:0.8538, val_acc:68.74%\n",
      " Epoch 22: iteration 106/107\n",
      "Epoch 22 --> loss:0.6710, acc:75.04%, val_loss:0.9141, val_acc:68.28%\n",
      " Epoch 23: iteration 106/107\n",
      "Epoch 23 --> loss:0.6452, acc:75.68%, val_loss:0.9006, val_acc:69.20%\n",
      " Epoch 24: iteration 106/107\n",
      "Epoch 24 --> loss:0.6352, acc:76.32%, val_loss:0.9263, val_acc:70.11%\n",
      " Epoch 25: iteration 106/107\n",
      "Epoch 25 --> loss:0.5873, acc:78.00%, val_loss:0.8246, val_acc:72.87%\n",
      " Epoch 26: iteration 106/107\n",
      "Epoch 26 --> loss:0.5577, acc:79.57%, val_loss:0.8303, val_acc:72.41%\n",
      " Epoch 27: iteration 106/107\n",
      "Epoch 27 --> loss:0.5493, acc:79.51%, val_loss:0.8584, val_acc:73.79%\n",
      " Epoch 28: iteration 106/107\n",
      "Epoch 28 --> loss:0.4793, acc:81.33%, val_loss:0.7320, val_acc:75.86%\n",
      " Epoch 29: iteration 106/107\n",
      "Epoch 29 --> loss:0.4645, acc:82.46%, val_loss:0.5522, val_acc:80.23%\n",
      " Epoch 30: iteration 106/107\n",
      "Epoch 30 --> loss:0.4397, acc:83.39%, val_loss:0.8937, val_acc:68.97%\n",
      " Epoch 31: iteration 106/107\n",
      "Epoch 31 --> loss:0.4046, acc:84.96%, val_loss:0.6233, val_acc:77.24%\n",
      " Epoch 32: iteration 106/107\n",
      "Epoch 32 --> loss:0.3996, acc:84.32%, val_loss:0.5760, val_acc:79.77%\n",
      " Epoch 33: iteration 106/107\n",
      "Epoch 33 --> loss:0.3973, acc:85.65%, val_loss:0.5021, val_acc:80.46%\n",
      " Epoch 34: iteration 106/107\n",
      "Epoch 34 --> loss:0.3561, acc:86.87%, val_loss:0.6196, val_acc:78.85%\n",
      " Epoch 35: iteration 106/107\n",
      "Epoch 35 --> loss:0.3197, acc:88.55%, val_loss:0.7684, val_acc:77.47%\n",
      " Epoch 36: iteration 106/107\n",
      "Epoch 36 --> loss:0.3328, acc:87.88%, val_loss:0.5907, val_acc:79.31%\n",
      " Epoch 37: iteration 106/107\n",
      "Epoch 37 --> loss:0.3157, acc:88.20%, val_loss:0.4843, val_acc:82.99%\n",
      " Epoch 38: iteration 106/107\n",
      "Epoch 38 --> loss:0.2711, acc:89.45%, val_loss:0.4606, val_acc:83.68%\n",
      " Epoch 39: iteration 106/107\n",
      "Epoch 39 --> loss:0.2935, acc:88.41%, val_loss:0.4698, val_acc:84.83%\n",
      " Epoch 40: iteration 106/107\n",
      "Epoch 40 --> loss:0.2783, acc:89.71%, val_loss:0.4489, val_acc:84.37%\n",
      " Epoch 41: iteration 106/107\n",
      "Epoch 41 --> loss:0.2352, acc:91.22%, val_loss:0.4450, val_acc:87.82%\n",
      " Epoch 42: iteration 106/107\n",
      "Epoch 42 --> loss:0.2715, acc:89.59%, val_loss:0.4472, val_acc:85.06%\n",
      " Epoch 43: iteration 106/107\n",
      "Epoch 43 --> loss:0.2497, acc:90.61%, val_loss:0.5598, val_acc:84.14%\n",
      " Epoch 44: iteration 106/107\n",
      "Epoch 44 --> loss:0.2229, acc:91.51%, val_loss:0.4010, val_acc:88.28%\n",
      " Epoch 45: iteration 106/107\n",
      "Epoch 45 --> loss:0.1960, acc:92.78%, val_loss:0.5793, val_acc:81.61%\n",
      " Epoch 46: iteration 106/107\n",
      "Epoch 46 --> loss:0.2092, acc:92.26%, val_loss:0.5206, val_acc:84.60%\n",
      " Epoch 47: iteration 106/107\n",
      "Epoch 47 --> loss:0.2331, acc:90.81%, val_loss:0.6826, val_acc:80.46%\n",
      " Epoch 48: iteration 106/107\n",
      "Epoch 48 --> loss:0.1803, acc:93.04%, val_loss:0.4869, val_acc:84.37%\n",
      " Epoch 49: iteration 106/107\n",
      "Epoch 49 --> loss:0.2165, acc:91.54%, val_loss:0.5125, val_acc:82.53%\n",
      " Epoch 50: iteration 106/107\n",
      "Epoch 50 --> loss:0.2094, acc:92.09%, val_loss:0.4005, val_acc:86.44%\n",
      " Epoch 51: iteration 106/107\n",
      "Epoch 51 --> loss:0.1299, acc:95.48%, val_loss:0.3841, val_acc:89.43%\n",
      " Epoch 52: iteration 106/107\n",
      "Epoch 52 --> loss:0.1439, acc:94.46%, val_loss:0.3504, val_acc:88.97%\n",
      " Epoch 53: iteration 106/107\n",
      "Epoch 53 --> loss:0.1995, acc:91.80%, val_loss:0.3441, val_acc:88.74%\n",
      " Epoch 54: iteration 106/107\n",
      "Epoch 54 --> loss:0.1754, acc:93.19%, val_loss:0.3281, val_acc:90.11%\n",
      " Epoch 55: iteration 106/107\n",
      "Epoch 55 --> loss:0.1411, acc:94.72%, val_loss:0.4362, val_acc:88.05%\n",
      " Epoch 56: iteration 106/107\n",
      "Epoch 56 --> loss:0.1247, acc:94.90%, val_loss:0.4406, val_acc:85.06%\n",
      " Epoch 57: iteration 106/107\n",
      "Epoch 57 --> loss:0.1411, acc:94.72%, val_loss:0.2977, val_acc:90.34%\n",
      " Epoch 58: iteration 106/107\n",
      "Epoch 58 --> loss:0.1081, acc:95.71%, val_loss:0.3516, val_acc:89.43%\n",
      " Epoch 59: iteration 106/107\n",
      "Epoch 59 --> loss:0.1246, acc:95.33%, val_loss:0.2951, val_acc:89.66%\n",
      " Epoch 60: iteration 106/107\n",
      "Epoch 60 --> loss:0.1383, acc:94.93%, val_loss:0.3929, val_acc:88.51%\n",
      " Epoch 61: iteration 106/107\n",
      "Epoch 61 --> loss:0.0969, acc:96.09%, val_loss:0.2727, val_acc:91.03%\n",
      " Epoch 62: iteration 106/107\n",
      "Epoch 62 --> loss:0.1059, acc:96.00%, val_loss:0.6558, val_acc:80.92%\n",
      " Epoch 63: iteration 106/107\n",
      "Epoch 63 --> loss:0.1272, acc:94.61%, val_loss:0.2902, val_acc:90.57%\n",
      " Epoch 64: iteration 106/107\n",
      "Epoch 64 --> loss:0.0953, acc:96.14%, val_loss:0.2883, val_acc:89.20%\n",
      " Epoch 65: iteration 106/107\n",
      "Epoch 65 --> loss:0.0880, acc:96.41%, val_loss:0.3097, val_acc:91.26%\n",
      " Epoch 66: iteration 106/107\n",
      "Epoch 66 --> loss:0.0691, acc:97.25%, val_loss:0.2878, val_acc:91.49%\n",
      " Epoch 67: iteration 106/107\n",
      "Epoch 67 --> loss:0.1338, acc:94.70%, val_loss:0.3454, val_acc:88.97%\n",
      " Epoch 68: iteration 106/107\n",
      "Epoch 68 --> loss:0.0833, acc:96.90%, val_loss:0.2626, val_acc:91.49%\n",
      " Epoch 69: iteration 106/107\n",
      "Epoch 69 --> loss:0.0927, acc:96.20%, val_loss:0.3012, val_acc:89.43%\n",
      " Epoch 70: iteration 106/107\n",
      "Epoch 70 --> loss:0.1277, acc:95.13%, val_loss:0.4341, val_acc:87.36%\n",
      " Epoch 71: iteration 106/107\n",
      "Epoch 71 --> loss:0.1420, acc:93.94%, val_loss:0.2340, val_acc:91.26%\n",
      " Epoch 72: iteration 106/107\n",
      "Epoch 72 --> loss:0.0736, acc:96.75%, val_loss:0.4836, val_acc:85.29%\n",
      " Epoch 73: iteration 106/107\n",
      "Epoch 73 --> loss:0.0858, acc:96.43%, val_loss:0.2123, val_acc:92.64%\n",
      " Epoch 74: iteration 106/107\n",
      "Epoch 74 --> loss:0.0558, acc:97.77%, val_loss:0.2942, val_acc:90.11%\n",
      " Epoch 75: iteration 106/107\n",
      "Epoch 75 --> loss:0.0703, acc:97.25%, val_loss:0.1738, val_acc:94.02%\n",
      " Epoch 76: iteration 106/107\n",
      "Epoch 76 --> loss:0.0607, acc:97.51%, val_loss:0.2967, val_acc:91.26%\n",
      " Epoch 77: iteration 106/107\n",
      "Epoch 77 --> loss:0.1139, acc:95.51%, val_loss:0.2741, val_acc:92.18%\n",
      " Epoch 78: iteration 106/107\n",
      "Epoch 78 --> loss:0.0546, acc:97.88%, val_loss:0.2882, val_acc:90.57%\n",
      " Epoch 79: iteration 106/107\n",
      "Epoch 79 --> loss:0.0678, acc:97.39%, val_loss:0.3832, val_acc:88.28%\n",
      " Epoch 80: iteration 106/107\n",
      "Epoch 80 --> loss:0.0681, acc:97.01%, val_loss:0.2426, val_acc:92.18%\n",
      " Epoch 81: iteration 106/107\n",
      "Epoch 81 --> loss:0.0525, acc:98.09%, val_loss:0.2690, val_acc:91.49%\n",
      " Epoch 82: iteration 106/107\n",
      "Epoch 82 --> loss:0.0770, acc:96.96%, val_loss:0.2699, val_acc:90.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 83: iteration 106/107\n",
      "Epoch 83 --> loss:0.0424, acc:98.32%, val_loss:0.4124, val_acc:90.57%\n",
      " Epoch 84: iteration 106/107\n",
      "Epoch 84 --> loss:0.0617, acc:97.22%, val_loss:0.1910, val_acc:94.48%\n",
      " Epoch 85: iteration 106/107\n",
      "Epoch 85 --> loss:0.0387, acc:98.23%, val_loss:0.2104, val_acc:93.56%\n",
      " Epoch 86: iteration 106/107\n",
      "Epoch 86 --> loss:0.0637, acc:97.19%, val_loss:0.2487, val_acc:93.56%\n",
      " Epoch 87: iteration 106/107\n",
      "Epoch 87 --> loss:0.0476, acc:98.06%, val_loss:0.2423, val_acc:91.95%\n",
      " Epoch 88: iteration 106/107\n",
      "Epoch 88 --> loss:0.0524, acc:97.77%, val_loss:0.2537, val_acc:91.26%\n",
      " Epoch 89: iteration 106/107\n",
      "Epoch 89 --> loss:0.0744, acc:97.04%, val_loss:0.2324, val_acc:92.18%\n",
      " Epoch 90: iteration 106/107\n",
      "Epoch 90 --> loss:0.0574, acc:97.51%, val_loss:0.2553, val_acc:93.33%\n",
      " Epoch 91: iteration 106/107\n",
      "Epoch 91 --> loss:0.0330, acc:98.58%, val_loss:0.1840, val_acc:94.02%\n",
      " Epoch 92: iteration 106/107\n",
      "Epoch 92 --> loss:0.0206, acc:99.01%, val_loss:0.1602, val_acc:94.94%\n",
      " Epoch 93: iteration 106/107\n",
      "Epoch 93 --> loss:0.0240, acc:98.81%, val_loss:0.1829, val_acc:94.02%\n",
      " Epoch 94: iteration 106/107\n",
      "Epoch 94 --> loss:0.0290, acc:98.58%, val_loss:0.2650, val_acc:91.03%\n",
      " Epoch 95: iteration 106/107\n",
      "Epoch 95 --> loss:0.0326, acc:98.46%, val_loss:0.1743, val_acc:95.40%\n",
      " Epoch 96: iteration 106/107\n",
      "Epoch 96 --> loss:0.0233, acc:98.75%, val_loss:0.1694, val_acc:94.48%\n",
      " Epoch 97: iteration 106/107\n",
      "Epoch 97 --> loss:0.0866, acc:96.46%, val_loss:0.2587, val_acc:92.64%\n",
      " Epoch 98: iteration 106/107\n",
      "Epoch 98 --> loss:0.1068, acc:95.80%, val_loss:0.3320, val_acc:89.66%\n",
      " Epoch 99: iteration 106/107\n",
      "Epoch 99 --> loss:0.0986, acc:96.06%, val_loss:0.3149, val_acc:91.26%\n",
      " Epoch 100: iteration 106/107\n",
      "Epoch 100 --> loss:0.1489, acc:94.35%, val_loss:0.2713, val_acc:90.80%\n",
      " Epoch 101: iteration 106/107\n",
      "Epoch 101 --> loss:0.0839, acc:96.58%, val_loss:0.3101, val_acc:90.11%\n",
      " Epoch 102: iteration 106/107\n",
      "Epoch 102 --> loss:0.0575, acc:97.59%, val_loss:0.2526, val_acc:92.41%\n",
      " Epoch 103: iteration 106/107\n",
      "Epoch 103 --> loss:0.0531, acc:97.86%, val_loss:0.2875, val_acc:91.49%\n",
      " Epoch 104: iteration 106/107\n",
      "Epoch 104 --> loss:0.0538, acc:97.57%, val_loss:0.2802, val_acc:92.18%\n",
      " Epoch 105: iteration 106/107\n",
      "Epoch 105 --> loss:0.0286, acc:98.70%, val_loss:0.2247, val_acc:92.64%\n",
      " Epoch 106: iteration 106/107\n",
      "Epoch 106 --> loss:0.0198, acc:98.93%, val_loss:0.1509, val_acc:95.86%\n",
      " Epoch 107: iteration 106/107\n",
      "Epoch 107 --> loss:0.0209, acc:98.99%, val_loss:0.1780, val_acc:94.25%\n",
      " Epoch 108: iteration 106/107\n",
      "Epoch 108 --> loss:0.0270, acc:98.58%, val_loss:0.1892, val_acc:94.48%\n",
      " Epoch 109: iteration 106/107\n",
      "Epoch 109 --> loss:0.0367, acc:98.35%, val_loss:0.2270, val_acc:91.95%\n",
      " Epoch 110: iteration 106/107\n",
      "Epoch 110 --> loss:0.0341, acc:98.46%, val_loss:0.2425, val_acc:93.33%\n",
      " Epoch 111: iteration 106/107\n",
      "Epoch 111 --> loss:0.0261, acc:98.64%, val_loss:0.3348, val_acc:90.57%\n",
      " Epoch 112: iteration 106/107\n",
      "Epoch 112 --> loss:0.0401, acc:98.23%, val_loss:0.2485, val_acc:92.87%\n",
      " Epoch 113: iteration 106/107\n",
      "Epoch 113 --> loss:0.0329, acc:98.35%, val_loss:0.2250, val_acc:92.41%\n",
      " Epoch 114: iteration 106/107\n",
      "Epoch 114 --> loss:0.0231, acc:98.84%, val_loss:0.2125, val_acc:92.18%\n",
      " Epoch 115: iteration 106/107\n",
      "Epoch 115 --> loss:0.2237, acc:91.68%, val_loss:0.4509, val_acc:84.37%\n",
      " Epoch 116: iteration 106/107\n",
      "Epoch 116 --> loss:0.1343, acc:94.64%, val_loss:0.3437, val_acc:89.20%\n",
      " Epoch 117: iteration 106/107\n",
      "Epoch 117 --> loss:0.0780, acc:96.87%, val_loss:0.1593, val_acc:94.71%\n",
      " Epoch 118: iteration 106/107\n",
      "Epoch 118 --> loss:0.0448, acc:97.88%, val_loss:0.2744, val_acc:91.03%\n",
      " Epoch 119: iteration 106/107\n",
      "Epoch 119 --> loss:0.0405, acc:98.46%, val_loss:0.1739, val_acc:94.48%\n",
      " Epoch 120: iteration 1/107"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5cbcdfeb6fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mX_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mY_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mepoch_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mactual_batch_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mDATASET_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mactual_batch_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mDATASET_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d3daeae7f242>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# update parameters and zero gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs=1500\n",
    "train_size = X_train.shape[0]\n",
    "minibatch = 32\n",
    "# set network to train on GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Selected device is {}'.format(device))\n",
    "#instantiate model and move model to GPU for training\n",
    "model = ParallelModel(num_emotions=len(EMOTIONS)).to(device) \n",
    "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
    "OPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "\n",
    "# instantiate the training step function \n",
    "train_step = make_train_step(model, criterion, optimizer=OPTIMIZER)\n",
    "\n",
    "# instantiate the validation loop function\n",
    "validate = make_validate_fnc(model,criterion)\n",
    "\n",
    "# instantiate lists to hold scalar performance metrics to plot later\n",
    "losses=[]\n",
    "val_losses = []\n",
    "\n",
    "# create training loop for one complete epoch (entire training set)\n",
    "def train(optimizer, model, num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # set model to train phase\n",
    "        model.train()         \n",
    "\n",
    "        # create a progress bar\n",
    "        progress = ProgressMonitor(length=3450)\n",
    "        \n",
    "        # shuffle entire training set in each epoch to randomize minibatch order\n",
    "        ind = np.random.permutation(train_size) \n",
    "        \n",
    "        # shuffle the training set for each epoch:\n",
    "        X_train = X_train[ind,:,:,:] \n",
    "        Y_train = Y_train[ind]\n",
    "        \n",
    "        # instantiate scalar values to keep track of progress after each epoch so we can stop training when appropriate \n",
    "        epoch_acc = 0 \n",
    "        epoch_loss = 0\n",
    "        num_iterations = int(train_size / minibatch)\n",
    "        \n",
    "        # create a loop for each minibatch of 32 samples:\n",
    "        for i in range(num_iterations):\n",
    "            \n",
    "            # we have to track and update minibatch position for the current minibatch\n",
    "            # if we take a random batch position from a set, we almost certainly will skip some of the data in that set\n",
    "            # track minibatch position based on iteration number:\n",
    "            batch_start = i * minibatch \n",
    "            # ensure we don't go out of the bounds of our training set:\n",
    "            batch_end = min(batch_start + minibatch, train_size) \n",
    "            # ensure we don't have an index error\n",
    "            actual_batch_size = batch_end-batch_start \n",
    "            \n",
    "            # get training minibatch with all channnels and 2D feature dims\n",
    "            X = X_train[batch_start:batch_end,:,:,:] \n",
    "            # get training minibatch labels \n",
    "            Y = Y_train[batch_start:batch_end] \n",
    "            \n",
    "            # instantiate training tensors\n",
    "            X_tensor = torch.tensor(X, device=device).float() \n",
    "            Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n",
    "            \n",
    "            # forward pass input tensors thru 1 training loop \n",
    "            loss, acc = train_step(X_tensor,Y_tensor) \n",
    "            \n",
    "            # aggregate accuracy to measure progress of entire epoch\n",
    "            epoch_acc += acc * actual_batch_size / train_size\n",
    "            epoch_loss += loss * actual_batch_size / train_size\n",
    "            print(f\"\\r Epoch {epoch}: iteration {i}/{num_iterations}\",end='')\n",
    "        \n",
    "        # create tensors from validation set\n",
    "        X_valid_tensor = torch.tensor(X_valid,device=device).float()\n",
    "        Y_valid_tensor = torch.tensor(Y_valid,dtype=torch.long,device=device)\n",
    "        \n",
    "        # calculate validation metrics to keep track of progress\n",
    "        valid_loss, valid_acc, _ = validate(X_valid_tensor,Y_valid_tensor)\n",
    "        # accumulate scalar performance metrics at each epoch to track and plot later\n",
    "        train_losses.append(epoch_loss)\n",
    "        valid_losses.append(val_loss)\n",
    "        \n",
    "        # keep track of each epoch's progress\n",
    "        print(f\"Epoch {epoch}: loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, val_acc:{val_acc:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is saved to /kaggle/working/models/cnn_transf_parallel_model.pt\n"
     ]
    }
   ],
   "source": [
    "save_path = '.\\models\\cnn_transformer'#os.path.join(os.getcwd(),'models\\')\n",
    "os.makedirs('models',exist_ok=True)\n",
    "torch.save(model.state_dict(),os.path.join(save_path,'transformer_cnn.pt'))\n",
    "print('fSaved model.pt to {}'.format(os.path.join(save_path,'transformer_cnn.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH = SAVE_PATH #os.path.join(os.getcwd(),'models')\n",
    "model = ParallelModel(len(EMOTIONS))\n",
    "model.load_state_dict(torch.load(os.path.join(LOAD_PATH,'transformer_cnn.pt')))\n",
    "print('Model is loaded from {}'.format(os.path.join(LOAD_PATH,'transformer_cnn.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss is 0.092\n",
      "Test accuracy is 96.78%\n"
     ]
    }
   ],
   "source": [
    "X_test_tensor = torch.tensor(X_test,device=device).float()\n",
    "Y_test_tensor = torch.tensor(Y_test,dtype=torch.long,device=device)\n",
    "test_loss, test_acc, predictions = validate(X_test_tensor,Y_test_tensor)\n",
    "print(f'Test loss is {test_loss:.3f}')\n",
    "print(f'Test accuracy is {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "predictions = predictions.cpu().numpy()\n",
    "cm = confusion_matrix(Y_test, predictions)\n",
    "names = [EMOTIONS[ind] for ind in range(len(EMOTIONS))]\n",
    "df_cm = pd.DataFrame(cm, index=names, columns=names)\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f263e518c50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses,'b')\n",
    "plt.plot(val_losses,'r')\n",
    "plt.legend(['train loss','val loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KERNEL",
   "language": "python",
   "name": "kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
